# Building client applications

Axflow provides some utilities for building client applications that consume model output. Currently, we offer React hooks and a [parser for consuming the streams](/guides/models/streaming.md#nd-json-stream) generated by `StreamingJsonResponse`.

This guide will cover the react hooks. You may also find the [streaming chat app tutorial](/tutorials/stream-chat-app.md) useful.

## Overview

Axflow provides a React hook called `useChat` that makes integrating streaming or non-streaming chat functionality trivial. Here we give an overview of the basic pattern.

First, we assume you have some API endpoint that either returns a JSON response or streams data using `StreamingJsonResponse`:

```ts
import { OpenAIChat } from '@axflow/models/openai/chat'
import { StreamingJsonResponse } from '@axflow/models/shared'

// POST /api/chat
export async function POST(request: Request) {
  const { query } = await request.json()

  const stream = await OpenAIChat.streamTokens(
    // args go here
  )

  // Uses a StreamingJsonResponse
  return new StreamingJsonResponse(stream)
}
```

Here we have an endpoint defined at `/api/chat` that uses our `StreamingJsonResponse` pattern. On the client, we can consume the streaming LLM response with the `useChat` component:

```ts
import { useChat } from '@axflow/models/react'

function ChatComponent() {
  const {input, messages, onChange, onSubmit} = useChat()

  return (
    <>
      <Messages messages={messages} />
      <Form input={input} onChange={onChange} onSubmit={onSubmit} />
    </>
  )
}
```

For simple cases, the defaults are all that are needed. See [Customizing `useChat`](#customizing-usechat) for overriding the defaults.

## Messages

The `useChat` hook creates and manages "message" objects. These are exposed as `messages` from the hook invocation.

```ts
const {messages} = useChat()
```

A message has the following type

```ts
type MessageType = {
  id: string;
  role: 'user' | 'assistant';
  data?: JSONValueType[];
  content: string;
  created: number;
}
```

and can be imported using the following:

```ts
import type {MessageType} from '@axflow/models/shared'
```

## Customizing `useChat`

`useChat` can be configured with a variety of options to control behavior.

### HTTP configuration

The default API endpoint is `/api/chat`, but you can configure it however you'd like. HTTP headers can also be supplied.

```ts
useChat({
  url: "https://your-site.com/your/arbitrary/chat/endpoint",
  headers: {
    'x-custom-header': '<custom-value>',
  }
})
```

### The request body

By default, the request body sent to the endpoint is:

```ts
type RequestBody = {
  messages: MessageType[],
}
```

This can be customized in two ways. First, you can easily add additional properties to be merged into the request body.

```ts
useChat({
  body: { user_id: user.id, stream: false }
})
```

The request will now contain the `user_id` and `stream` properties in addition to the `messages` property.

Second, you can pass a function. The return value of the function will become the request body.

```ts
useChat({
  body: (message: MessageType, messageHistory: MessageType[]) => {
    return {message, history: messageHistory, user_id: user.id, stream: false}
  }
})
```

The function takes the current message being created as the first argument and all previous messages in the chat history as the second.

### Accessing the the LLM response text

Your API endpoint may return objects with any shape. However, the hook will need to know how to access the response text generated by the LLM.

For example, let's say your application streams back objects with the following schema:

```json
{ id: 1, is_finished: false, token: "The" }
{ id: 2, is_finished: false, token: " response" }
...
{ id: 10, is_finished: true, token: "." }
```

We need to tell the `useChat` hook that the `token` property of each chunk contains the text we wish to display to the user. We can do that using the `accessor` option.

```ts
useChat({
  accessor: (chunk) => chunk.token
})
```

This option takes a function which is given either a chunk (in the case of streaming) or the response body (in case of non-streaming).

### Message and input state

`useChat` will manage the user's input and messages for you. However, you may wish to initialize the hook with pre-existing state, or manually reset this state.

To initialize this state, you can use the `initialInput` or `initialMessages` options.

```ts
useChat({
  initialInput: savedUserInput(),
  initialMessages: savedMessageHistory(),
})
```

To reset this state manually, you may use the `setInput` or `setMessages` functions.

```tsx
const {setInput, setMessages, /* etc */} = useChat({ /* options */ });

// ...

<ClearInput onClick={() => setInput('')} />
<ClearMessageHistory onClick={() => setMessages([])} />
```

## Streaming vs non-streaming

The hook supports both streaming and non-streaming. If you wish to use streaming, your API MUST:

1. Include a `content-type` response header set to `application/x-ndjson; charset=utf-8`
2. Stream chunks as newline-delimited JSON where each line of JSON uses a specific schema

For 1, if the `content-type` header is anything but `application/x-ndjson; charset=utf-8`, the hook will assume this is a non-streaming response.

For 2, the schema is:

```ts
type NdJsonValueType = {
  type: 'chunk' | 'data';
  value: JsonValueType; // any valid JSON value here
}
```

If you're using `StreamingJsonResponse`, this is handled for you. See the [streaming guide](/guides/models/streaming.md) for more information on `StreamingJsonResponse` and its output.
